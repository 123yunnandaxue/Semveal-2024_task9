{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21997,"status":"ok","timestamp":1706706332989,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"},"user_tz":-480},"id":"TAVmgNxYPp2d","outputId":"beefe496-e231-4b18-9c83-e9fb038ea525"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}],"source":["!pip install datasets\n","!pip install transformers\n","!pip install transformers accelerate evaluate datasets peft -q\n","!pip install --no-cache-dir transformers sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_4b8HbfOx2g"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datasets\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, BertForMultipleChoice,DataCollatorWithPadding\n","from transformers import Trainer, TrainingArguments\n","import os\n","import sys\n","import logging\n","import torch\n","SP_train=pd.read_csv('/content/WP_train_translation.csv')\n","\n","SP_train=SP_train.drop(columns='Unnamed: 0')\n","\n","def create_choice_list(data):\n","    choice_list=[]\n","    for i in range(len(data['question'])):\n","        choice_list.append([data['A'][i],data['B'][i],data['C'][i],data['D'][i]])\n","    return choice_list\n","SP_train['choice_list']=create_choice_list(SP_train)\n","\n","SP_train=SP_train.drop(columns=['A','B','C','D'])\n","\n","\n","train, val = train_test_split(pd.DataFrame(SP_train), test_size=.2)\n","train=train.to_dict('list')\n","val=val.to_dict('list')\n","\n","train_dataset = datasets.Dataset.from_dict(train)\n","val_dataset = datasets.Dataset.from_dict(val)\n","\n","def deal(examples):\n","    key=examples[0].keys()\n","    question=[]\n","    choice_list=[]\n","    label=[]\n","    for i in range(len(examples)):\n","        if 'label' in key:\n","            label.append(int(examples[i]['label']))\n","        question.append(examples[i]['question'])\n","        choice_list.append(examples[i]['choice_list'])\n","    if 'label' in key:\n","        data_dict={'question':question,'choice_list':choice_list,'label':label}\n","    else:\n","        data_dict={'question':question,'choice_list':choice_list}\n","    return data_dict\n","\n","\n","def preprocess_function(examples):\n","    prompts=[]\n","    inputs=[]\n","    for i in range(len(examples['question'])):\n","        encodings=tokenizer([examples['question'][i]]*4, examples['choice_list'][i],\n","                            truncation=True,padding='max_length',max_length=175,return_tensors=\"pt\")\n","        inputs.append(encodings)\n","    input_ids = torch.stack([x['input_ids'] for x in inputs])\n","    attention_mask = torch.stack([x['attention_mask'] for x in inputs])\n","    token_type_ids = torch.stack([x['token_type_ids'] for x in inputs])\n","    return {'input_ids':input_ids,'attention_mask':attention_mask,'token_type_ids':token_type_ids}\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), preds.flatten(), average='weighted', zero_division=0)\n","    return {\n","        'accuracy': (preds == eval_pred.label_ids).mean(),\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","SP_test=np.load('/content/WP_new_test.npy',allow_pickle=True)\n","\n","test_dict=deal(SP_test)\n","\n","test_dataset=datasets.Dataset.from_dict(test_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TxeWxhbr_iM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torchvision\n","import torchvision.transforms as F\n","\n","from IPython.display import display\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, weight=None, reduction='mean', gamma=0, eps=1e-7):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.eps = eps\n","        self.ce = torch.nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n","\n","    def forward(self, input, target):\n","        logp = self.ce(input, target)\n","        p = torch.exp(-logp)\n","        loss = (1 - p) ** self.gamma * logp\n","        return loss.mean()\n","\n","\n","\n","from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","from transformers import DebertaV2PreTrainedModel, DebertaV2Model\n","from typing import Optional, Tuple, Union\n","\n","from transformers.modeling_outputs import MultipleChoiceModelOutput\n","from transformers.activations import ACT2FN\n","\n","class ContextPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.pooler_hidden_size, config.pooler_hidden_size)\n","        self.dropout = StableDropout(config.pooler_dropout)\n","        self.config = config\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","\n","        context_token = hidden_states[:, 0]\n","        context_token = self.dropout(context_token)\n","        pooled_output = self.dense(context_token)\n","        pooled_output = ACT2FN[self.config.pooler_hidden_act](pooled_output)\n","        return pooled_output\n","\n","    @property\n","    def output_dim(self):\n","        return self.config.hidden_size\n","\n","class StableDropout(nn.Module):\n","    \"\"\"\n","    Optimized dropout module for stabilizing the training\n","\n","    Args:\n","        drop_prob (float): the dropout probabilities\n","    \"\"\"\n","\n","    def __init__(self, drop_prob):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Call the module\n","\n","        Args:\n","            x (`torch.tensor`): The input tensor to apply dropout\n","        \"\"\"\n","        if self.training and self.drop_prob > 0:\n","            return XDropout.apply(x, self.get_context())\n","        return x\n","\n","    def clear_context(self):\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def init_context(self, reuse_mask=True, scale=1):\n","        if self.context_stack is None:\n","            self.context_stack = []\n","        self.count = 0\n","        for c in self.context_stack:\n","            c.reuse_mask = reuse_mask\n","            c.scale = scale\n","\n","    def get_context(self):\n","        if self.context_stack is not None:\n","            if self.count >= len(self.context_stack):\n","                self.context_stack.append(DropoutContext())\n","            ctx = self.context_stack[self.count]\n","            ctx.dropout = self.drop_prob\n","            self.count += 1\n","            return ctx\n","        else:\n","            return self.drop_prob\n","\n","class DropoutContext(object):\n","    def __init__(self):\n","        self.dropout = 0\n","        self.mask = None\n","        self.scale = 1\n","        self.reuse_mask = True\n","\n","\n","# Copied from transformers.models.deberta.modeling_deberta.get_mask\n","def get_mask(input, local_context):\n","    if not isinstance(local_context, DropoutContext):\n","        dropout = local_context\n","        mask = None\n","    else:\n","        dropout = local_context.dropout\n","        dropout *= local_context.scale\n","        mask = local_context.mask if local_context.reuse_mask else None\n","\n","    if dropout > 0 and mask is None:\n","        mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).to(torch.bool)\n","\n","    if isinstance(local_context, DropoutContext):\n","        if local_context.mask is None:\n","            local_context.mask = mask\n","\n","    return mask, dropout\n","class XDropout(torch.autograd.Function):\n","    \"\"\"Optimized dropout function to save computation and memory by using mask operation instead of multiplication.\"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, local_ctx):\n","        mask, dropout = get_mask(input, local_ctx)\n","        ctx.scale = 1.0 / (1 - dropout)\n","        if dropout > 0:\n","            ctx.save_for_backward(mask)\n","            return input.masked_fill(mask, 0) * ctx.scale\n","        else:\n","            return input\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        if ctx.scale > 1:\n","            (mask,) = ctx.saved_tensors\n","            return grad_output.masked_fill(mask, 0) * ctx.scale, None\n","        else:\n","            return grad_output, None\n","\n","    @staticmethod\n","    def symbolic(g: torch._C.Graph, input: torch._C.Value, local_ctx: Union[float, DropoutContext]) -> torch._C.Value:\n","        from torch.onnx import symbolic_opset12\n","\n","        dropout_p = local_ctx\n","        if isinstance(local_ctx, DropoutContext):\n","            dropout_p = local_ctx.dropout\n","        # StableDropout only calls this function when training.\n","        train = True\n","        # TODO: We should check if the opset_version being used to export\n","        # is > 12 here, but there's no good way to do that. As-is, if the\n","        # opset_version < 12, export will fail with a CheckerError.\n","        # Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:\n","        # if opset_version < 12:\n","        #   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)\n","        return symbolic_opset12.dropout(g, input, dropout_p, train)\n","\n","\n","class DebertaV2ForMultipleChoice(DebertaV2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        num_labels = getattr(config, \"num_labels\", 2)\n","        self.num_labels = num_labels\n","\n","        self.deberta = DebertaV2Model(config)\n","        self.pooler = ContextPooler(config)\n","        output_dim = self.pooler.output_dim\n","\n","        self.classifier = nn.Linear(output_dim, 1)\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = StableDropout(drop_out)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.deberta.get_input_embeddings()\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.deberta.set_input_embeddings(new_embeddings)\n","\n","    # @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n","    # @add_code_sample_docstrings(\n","    #     checkpoint=_CHECKPOINT_FOR_DOC,\n","    #     output_type=MultipleChoiceModelOutput,\n","    #     config_class=_CONFIG_FOR_DOC,\n","    # )\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, MultipleChoiceModelOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n","            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n","            `input_ids` above)\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n","\n","        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n","        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        flat_inputs_embeds = (\n","            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n","            if inputs_embeds is not None\n","            else None\n","        )\n","\n","        outputs = self.deberta(\n","            flat_input_ids,\n","            position_ids=flat_position_ids,\n","            token_type_ids=flat_token_type_ids,\n","            attention_mask=flat_attention_mask,\n","            inputs_embeds=flat_inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        encoder_layer = outputs[0]\n","        pooled_output = self.pooler(encoder_layer)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = FocalLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","\n","        if not return_dict:\n","            output = (reshaped_logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return MultipleChoiceModelOutput(\n","            loss=loss,\n","            logits=reshaped_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o48LJuB3sCcy","executionInfo":{"status":"ok","timestamp":1706706433058,"user_tz":-480,"elapsed":92935,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}},"outputId":"5e111f00-9111-4f74-9db2-ee42584287ac"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from sklearn.metrics import precision_recall_fscore_support\n","import warnings\n","import peft\n","from peft import LoraConfig, get_peft_model,prepare_model_for_int8_training\n","import evaluate\n","warnings.filterwarnings('ignore')\n","check_point=\"microsoft/deberta-v2-xxlarge\"\n","tokenizer = AutoTokenizer.from_pretrained(check_point)\n","model = DebertaV2ForMultipleChoice.from_pretrained(check_point)\n","\n","tokenized_train = preprocess_function(train_dataset)\n","tokenized_val = preprocess_function(val_dataset)\n","tokenized_test = preprocess_function(test_dataset)\n","labels01=torch.Tensor(train_dataset['label']).cuda()\n","labels02=torch.Tensor(val_dataset['label']).cuda()\n","tokenized_train['label']=labels01.int()\n","tokenized_val['label']=labels02.int()\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","metric = evaluate.load(\"accuracy\")\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    task_type = \"SEQ_CLS\",\n","    target_modules=[\"query_proj\", \"value_proj\"],\n",")\n","\n","model = prepare_model_for_int8_training(model)\n","# 转换模型\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWyNZ6QBsCfd"},"outputs":[],"source":["training_args = TrainingArguments(\n","        output_dir='./checkpoint',  # output directory\n","        num_train_epochs=5,  # total number of training epochs\n","        per_device_train_batch_size=1,  # batch size per device during training\n","        per_device_eval_batch_size=1,  # batch size for evaluation\n","        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n","        weight_decay=0.01,  # strength of weight decay\n","        logging_dir='./logs',  # directory for storing logs\n","        logging_steps=100,\n","        save_strategy='epoch',\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=5e-5,\n","        fp16=True,\n","        remove_unused_columns=False,\n","        load_best_model_at_end=True\n","\n",")\n","\n","trainer = Trainer(\n","        model=model,  # the instantiated 🤗 Transformers model to be trained\n","        args=training_args,  # training arguments, defined above\n","        train_dataset=datasets.Dataset.from_dict(tokenized_train),  # training dataset\n","        eval_dataset=datasets.Dataset.from_dict(tokenized_val),  # evaluation dataset\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4wQOqaUsChz","colab":{"base_uri":"https://localhost:8080/","height":535},"outputId":"dda323cb-7e55-4335-bba3-76e97bdea0a8","executionInfo":{"status":"error","timestamp":1706709796637,"user_tz":-480,"elapsed":3360713,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6940' max='15840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 6940/15840 55:57 < 1:11:47, 2.07 it/s, Epoch 2.19/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.453500</td>\n","      <td>1.386608</td>\n","      <td>0.303030</td>\n","      <td>0.162472</td>\n","      <td>0.258006</td>\n","      <td>0.303030</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.424000</td>\n","      <td>1.386473</td>\n","      <td>0.313131</td>\n","      <td>0.189467</td>\n","      <td>0.239306</td>\n","      <td>0.313131</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","source":["print(trainer.evaluate())\n","prediction_outputs = trainer.predict(datasets.Dataset.from_dict(tokenized_test))\n","test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n","print(test_pred)"],"metadata":{"id":"fv9bcUegCZvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(test_pred)"],"metadata":{"id":"J051_3v8iD44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sp_train=np.load('/content/WP-train.npy',allow_pickle=True)\n","train_dict=deal(sp_train)\n","\n","train, val = train_test_split(pd.DataFrame(train_dict), test_size=.2)\n","train=train.to_dict('list')\n","val=val.to_dict('list')\n","train_dataset = datasets.Dataset.from_dict(train)\n","val_dataset = datasets.Dataset.from_dict(val)\n","\n","tokenized_train = preprocess_function(train_dataset)\n","tokenized_val = preprocess_function(val_dataset)\n","\n","labels01=torch.Tensor(train_dataset['label']).cuda()\n","labels02=torch.Tensor(val_dataset['label']).cuda()\n","tokenized_train['label']=labels01.int()\n","tokenized_val['label']=labels02.int()\n","\n","trainer = Trainer(\n","        model=model,  # the instantiated 🤗 Transformers model to be trained\n","        args=training_args,  # training arguments, defined above\n","        train_dataset=datasets.Dataset.from_dict(tokenized_train),  # training dataset\n","        eval_dataset=datasets.Dataset.from_dict(tokenized_val),  # evaluation dataset\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","trainer.train()\n"],"metadata":{"id":"ezbb4osG5nai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ghv0T-G5MH7a"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"V100","machine_shape":"hm","authorship_tag":"ABX9TyPKhTe8YVqE91kzMX8T4Jp+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}