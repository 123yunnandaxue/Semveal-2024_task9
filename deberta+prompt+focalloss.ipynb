{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNzWjdL8PDkb7KXasZgr1ba"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n","!pip install transformers accelerate evaluate datasets peft -q\n","!pip install --no-cache-dir transformers sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ny1fdVFLnXgD","executionInfo":{"status":"ok","timestamp":1702645130011,"user_tz":-480,"elapsed":43391,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}},"outputId":"f93c8f49-2d5c-4205-ca36-7672f46aeb00"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import torchvision\n","import torchvision.transforms as F\n","\n","from IPython.display import display\n","class FocalLoss(nn.Module):\n","\n","    def __init__(self, weight=None, reduction='mean', gamma=0, eps=1e-7):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.eps = eps\n","        self.ce = torch.nn.CrossEntropyLoss(weight=weight, reduction=reduction)\n","\n","    def forward(self, input, target):\n","        logp = self.ce(input, target)\n","        p = torch.exp(-logp)\n","        loss = (1 - p) ** self.gamma * logp\n","        return loss.mean()"],"metadata":{"id":"k6ir1Zl2bgY3","executionInfo":{"status":"ok","timestamp":1702645135743,"user_tz":-480,"elapsed":5751,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function\n","\n","import torch\n","import torch.nn as nn\n","from transformers import DebertaV2PreTrainedModel, DebertaV2Model\n","from typing import Optional, Tuple, Union\n","\n","from transformers.modeling_outputs import MultipleChoiceModelOutput\n","from transformers.activations import ACT2FN\n","\n","class ContextPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.pooler_hidden_size, config.pooler_hidden_size)\n","        self.dropout = StableDropout(config.pooler_dropout)\n","        self.config = config\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","\n","        context_token = hidden_states[:, 0]\n","        context_token = self.dropout(context_token)\n","        pooled_output = self.dense(context_token)\n","        pooled_output = ACT2FN[self.config.pooler_hidden_act](pooled_output)\n","        return pooled_output\n","\n","    @property\n","    def output_dim(self):\n","        return self.config.hidden_size\n","\n","class StableDropout(nn.Module):\n","    \"\"\"\n","    Optimized dropout module for stabilizing the training\n","\n","    Args:\n","        drop_prob (float): the dropout probabilities\n","    \"\"\"\n","\n","    def __init__(self, drop_prob):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Call the module\n","\n","        Args:\n","            x (`torch.tensor`): The input tensor to apply dropout\n","        \"\"\"\n","        if self.training and self.drop_prob > 0:\n","            return XDropout.apply(x, self.get_context())\n","        return x\n","\n","    def clear_context(self):\n","        self.count = 0\n","        self.context_stack = None\n","\n","    def init_context(self, reuse_mask=True, scale=1):\n","        if self.context_stack is None:\n","            self.context_stack = []\n","        self.count = 0\n","        for c in self.context_stack:\n","            c.reuse_mask = reuse_mask\n","            c.scale = scale\n","\n","    def get_context(self):\n","        if self.context_stack is not None:\n","            if self.count >= len(self.context_stack):\n","                self.context_stack.append(DropoutContext())\n","            ctx = self.context_stack[self.count]\n","            ctx.dropout = self.drop_prob\n","            self.count += 1\n","            return ctx\n","        else:\n","            return self.drop_prob\n","\n","class DropoutContext(object):\n","    def __init__(self):\n","        self.dropout = 0\n","        self.mask = None\n","        self.scale = 1\n","        self.reuse_mask = True\n","\n","\n","# Copied from transformers.models.deberta.modeling_deberta.get_mask\n","def get_mask(input, local_context):\n","    if not isinstance(local_context, DropoutContext):\n","        dropout = local_context\n","        mask = None\n","    else:\n","        dropout = local_context.dropout\n","        dropout *= local_context.scale\n","        mask = local_context.mask if local_context.reuse_mask else None\n","\n","    if dropout > 0 and mask is None:\n","        mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).to(torch.bool)\n","\n","    if isinstance(local_context, DropoutContext):\n","        if local_context.mask is None:\n","            local_context.mask = mask\n","\n","    return mask, dropout\n","class XDropout(torch.autograd.Function):\n","    \"\"\"Optimized dropout function to save computation and memory by using mask operation instead of multiplication.\"\"\"\n","\n","    @staticmethod\n","    def forward(ctx, input, local_ctx):\n","        mask, dropout = get_mask(input, local_ctx)\n","        ctx.scale = 1.0 / (1 - dropout)\n","        if dropout > 0:\n","            ctx.save_for_backward(mask)\n","            return input.masked_fill(mask, 0) * ctx.scale\n","        else:\n","            return input\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        if ctx.scale > 1:\n","            (mask,) = ctx.saved_tensors\n","            return grad_output.masked_fill(mask, 0) * ctx.scale, None\n","        else:\n","            return grad_output, None\n","\n","    @staticmethod\n","    def symbolic(g: torch._C.Graph, input: torch._C.Value, local_ctx: Union[float, DropoutContext]) -> torch._C.Value:\n","        from torch.onnx import symbolic_opset12\n","\n","        dropout_p = local_ctx\n","        if isinstance(local_ctx, DropoutContext):\n","            dropout_p = local_ctx.dropout\n","        # StableDropout only calls this function when training.\n","        train = True\n","        # TODO: We should check if the opset_version being used to export\n","        # is > 12 here, but there's no good way to do that. As-is, if the\n","        # opset_version < 12, export will fail with a CheckerError.\n","        # Once https://github.com/pytorch/pytorch/issues/78391 is fixed, do something like:\n","        # if opset_version < 12:\n","        #   return torch.onnx.symbolic_opset9.dropout(g, input, dropout_p, train)\n","        return symbolic_opset12.dropout(g, input, dropout_p, train)\n","\n","\n","class DebertaV2ForMultipleChoice(DebertaV2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        num_labels = getattr(config, \"num_labels\", 2)\n","        self.num_labels = num_labels\n","\n","        self.deberta = DebertaV2Model(config)\n","        self.pooler = ContextPooler(config)\n","        output_dim = self.pooler.output_dim\n","\n","        self.classifier = nn.Linear(output_dim, 1)\n","        drop_out = getattr(config, \"cls_dropout\", None)\n","        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n","        self.dropout = StableDropout(drop_out)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.deberta.get_input_embeddings()\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.deberta.set_input_embeddings(new_embeddings)\n","\n","    # @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n","    # @add_code_sample_docstrings(\n","    #     checkpoint=_CHECKPOINT_FOR_DOC,\n","    #     output_type=MultipleChoiceModelOutput,\n","    #     config_class=_CONFIG_FOR_DOC,\n","    # )\n","    def forward(\n","        self,\n","        input_ids: Optional[torch.Tensor] = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        token_type_ids: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","        inputs_embeds: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, MultipleChoiceModelOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n","            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n","            `input_ids` above)\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n","\n","        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n","        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n","        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n","        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n","        flat_inputs_embeds = (\n","            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n","            if inputs_embeds is not None\n","            else None\n","        )\n","\n","        outputs = self.deberta(\n","            flat_input_ids,\n","            position_ids=flat_position_ids,\n","            token_type_ids=flat_token_type_ids,\n","            attention_mask=flat_attention_mask,\n","            inputs_embeds=flat_inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        encoder_layer = outputs[0]\n","        pooled_output = self.pooler(encoder_layer)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        reshaped_logits = logits.view(-1, num_choices)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = FocalLoss()\n","            loss = loss_fct(reshaped_logits, labels)\n","\n","        if not return_dict:\n","            output = (reshaped_logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return MultipleChoiceModelOutput(\n","            loss=loss,\n","            logits=reshaped_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )"],"metadata":{"id":"gUbRXGLGbhrc","executionInfo":{"status":"ok","timestamp":1702645135744,"user_tz":-480,"elapsed":19,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"x5URZHOOmOFg","executionInfo":{"status":"ok","timestamp":1702645141042,"user_tz":-480,"elapsed":5317,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datasets\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, BertForMultipleChoice,DataCollatorWithPadding\n","from transformers import Trainer, TrainingArguments\n","import os\n","import sys\n","import logging\n","import torch\n","\n","from peft import PromptTuningConfig, get_peft_model, TaskType\n","\n","SP_train=np.load('/content/WP-train.npy',allow_pickle=True)\n","SP_test=np.load('/content/WP_eval_data_for_practice.npy',allow_pickle=True)\n","def deal(examples):\n","    key=examples[0].keys()\n","    question=[]\n","    choice_list=[]\n","    label=[]\n","    for i in range(len(examples)):\n","        if 'label' in key:\n","            label.append(int(examples[i]['label']))\n","        question.append(examples[i]['question'])\n","        choice_list.append(examples[i]['choice_list'])\n","    if 'label' in key:\n","        data_dict={'question':question,'choice_list':choice_list,'label':label}\n","    else:\n","        data_dict={'question':question,'choice_list':choice_list}\n","    return data_dict\n","def preprocess_function(examples):\n","    inputs=[]\n","    for i in range(len(examples['question'])):\n","        encodings=tokenizer([examples['question'][i]]*4, examples['choice_list'][i],\n","                            truncation=True,padding='max_length',max_length=96,return_tensors=\"pt\")\n","        inputs.append(encodings)\n","    input_ids = torch.stack([x['input_ids'] for x in inputs])\n","    attention_mask = torch.stack([x['attention_mask'] for x in inputs])\n","    token_type_ids = torch.stack([x['token_type_ids'] for x in inputs])\n","    return {'input_ids':input_ids,'attention_mask':attention_mask,'token_type_ids':token_type_ids}\n","train_dict=deal(SP_train)\n","test_dict=deal(SP_test)\n","train, val = train_test_split(pd.DataFrame(train_dict), test_size=.2)\n","train=train.to_dict('list')\n","val=val.to_dict('list')\n","train_dataset = datasets.Dataset.from_dict(train)\n","val_dataset = datasets.Dataset.from_dict(val)\n","test_dataset=datasets.Dataset.from_dict(test_dict)\n","# def compute_metrics(eval_pred):\n","#     logits, labels = eval_pred\n","#     predictions = np.argmax(logits, axis=-1)\n","#     return metric.compute(predictions=predictions, references=labels)\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = np.argmax(logits, axis=-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), preds.flatten(), average='weighted', zero_division=0)\n","    return {\n","        'accuracy': (preds == eval_pred.label_ids).mean(),\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n"]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","import warnings\n","import peft\n","from peft import LoraConfig, get_peft_model,prepare_model_for_int8_training\n","import evaluate\n","warnings.filterwarnings('ignore')\n","check_point=\"microsoft/deberta-v2-xxlarge\"\n","tokenizer = AutoTokenizer.from_pretrained(check_point,use_fast=False)\n","model = DebertaV2ForMultipleChoice.from_pretrained(check_point)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N268U489DOwn","executionInfo":{"status":"ok","timestamp":1702645187789,"user_tz":-480,"elapsed":46763,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}},"outputId":"3fedc8c2-f72d-440c-ba57-068fa3c00ab7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["tokenized_train = preprocess_function(train_dataset)\n","tokenized_val = preprocess_function(val_dataset)\n","tokenized_test = preprocess_function(test_dataset)\n","labels01=torch.Tensor(train_dataset['label']).cuda()\n","labels02=torch.Tensor(val_dataset['label']).cuda()\n","tokenized_train['label']=labels01.int()\n","tokenized_val['label']=labels02.int()\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"Z1Z1RnweDCo7","executionInfo":{"status":"ok","timestamp":1702645189582,"user_tz":-480,"elapsed":1820,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from __future__ import annotations\n","\n","import collections\n","import inspect\n","import os\n","import warnings\n","from contextlib import contextmanager\n","from copy import deepcopy\n","from typing import Any, Dict, List, Optional, Union\n","\n","import torch\n","from accelerate import dispatch_model, infer_auto_device_map\n","from accelerate.hooks import AlignDevicesHook, add_hook_to_module, remove_hook_from_submodules\n","from accelerate.utils import get_balanced_memory\n","from huggingface_hub import ModelCard, ModelCardData, hf_hub_download\n","from safetensors.torch import save_file as safe_save_file\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from transformers import PreTrainedModel\n","from transformers.modeling_outputs import QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput\n","from transformers.utils import PushToHubMixin\n","\n","\n","from peft.config import PeftConfig\n","from peft.tuners import (\n","    AdaLoraModel,\n","    AdaptionPromptModel,\n","    IA3Model,\n","    LoHaModel,\n","    LoKrModel,\n","    LoraModel,\n","    MultitaskPromptEmbedding,\n","    OFTModel,\n","    PrefixEncoder,\n","    PromptEmbedding,\n","    PromptEncoder,\n",")\n","from peft.utils import (\n","    SAFETENSORS_WEIGHTS_NAME,\n","    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n","    WEIGHTS_NAME,\n","    PeftType,\n","    TaskType,\n","    _get_batch_size,\n","    _prepare_prompt_learning_config,\n","    _set_adapter,\n","    _set_trainable,\n","    get_peft_model_state_dict,\n","    id_tensor_storage,\n","    infer_device,\n","    load_peft_weights,\n","    set_peft_model_state_dict,\n","    shift_tokens_right,\n",")\n","\n","\n","PEFT_TYPE_TO_MODEL_MAPPING = {\n","    PeftType.LORA: LoraModel,\n","    PeftType.LOHA: LoHaModel,\n","    PeftType.LOKR: LoKrModel,\n","    PeftType.PROMPT_TUNING: PromptEmbedding,\n","    PeftType.P_TUNING: PromptEncoder,\n","    PeftType.PREFIX_TUNING: PrefixEncoder,\n","    PeftType.ADALORA: AdaLoraModel,\n","    PeftType.ADAPTION_PROMPT: AdaptionPromptModel,\n","    PeftType.IA3: IA3Model,\n","    PeftType.OFT: OFTModel,\n","}"],"metadata":{"id":"SO4TTkgXW_qx","executionInfo":{"status":"ok","timestamp":1702645189618,"user_tz":-480,"elapsed":79,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from peft import PeftModel\n","\n","class PeftModelForSequenceClassification(PeftModel):\n","    \"\"\"\n","    Peft model for sequence classification tasks.\n","\n","    Args:\n","        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n","        peft_config ([`PeftConfig`]): Peft config.\n","\n","    **Attributes**:\n","        - **config** ([`~transformers.PretrainedConfig`]) -- The configuration object of the base model.\n","        - **cls_layer_name** (`str`) -- The name of the classification layer.\n","\n","    Example:\n","\n","        ```py\n","        >>> from transformers import AutoModelForSequenceClassification\n","        >>> from peft import PeftModelForSequenceClassification, get_peft_config\n","\n","        >>> config = {\n","        ...     \"peft_type\": \"PREFIX_TUNING\",\n","        ...     \"task_type\": \"SEQ_CLS\",\n","        ...     \"inference_mode\": False,\n","        ...     \"num_virtual_tokens\": 20,\n","        ...     \"token_dim\": 768,\n","        ...     \"num_transformer_submodules\": 1,\n","        ...     \"num_attention_heads\": 12,\n","        ...     \"num_layers\": 12,\n","        ...     \"encoder_hidden_size\": 768,\n","        ...     \"prefix_projection\": False,\n","        ...     \"postprocess_past_key_value_function\": None,\n","        ... }\n","\n","        >>> peft_config = get_peft_config(config)\n","        >>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n","        >>> peft_model = PeftModelForSequenceClassification(model, peft_config)\n","        >>> peft_model.print_trainable_parameters()\n","        trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n","        ```\n","    \"\"\"\n","\n","    def __init__(self, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: str = \"default\") -> None:\n","        super().__init__(model, peft_config, adapter_name)\n","        if self.modules_to_save is None:\n","            self.modules_to_save = {\"classifier\", \"score\"}\n","        else:\n","            self.modules_to_save.update({\"classifier\", \"score\"})\n","\n","        for name, _ in self.base_model.named_children():\n","            if any(module_name in name for module_name in self.modules_to_save):\n","                self.cls_layer_name = name\n","                break\n","\n","        # to make sure classifier layer is trainable\n","        _set_trainable(self, adapter_name)\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        task_ids=None,\n","        **kwargs,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        peft_config = self.active_peft_config\n","        if not peft_config.is_prompt_learning:\n","            return self.base_model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","                labels=labels,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","                **kwargs,\n","            )\n","\n","        batch_size = _get_batch_size(input_ids, inputs_embeds)\n","        if attention_mask is not None:\n","            # concat prompt attention mask\n","            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(attention_mask.device)\n","            prefix_attention_mask=prefix_attention_mask.unsqueeze(0)\n","            #print(prefix_attention_mask.shape)#[1,1,175]\n","            #print(attention_mask.shape)#[1,4,175]\n","            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n","        if kwargs.get(\"position_ids\", None) is not None:\n","            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n","            kwargs[\"position_ids\"] = None\n","        kwargs.update(\n","            {\n","                \"attention_mask\": attention_mask,\n","                \"labels\": labels,\n","                \"output_attentions\": output_attentions,\n","                \"output_hidden_states\": output_hidden_states,\n","                \"return_dict\": return_dict,\n","            }\n","        )\n","\n","        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n","            return self._prefix_tuning_forward(input_ids=input_ids, **kwargs)\n","        else:\n","            if kwargs.get(\"token_type_ids\", None) is not None:\n","                kwargs[\"token_type_ids\"] = torch.cat(\n","                    (\n","                        torch.zeros(batch_size, peft_config.num_virtual_tokens).unsqueeze(0).to(self.word_embeddings.weight.device),\n","                        kwargs[\"token_type_ids\"],\n","                    ),\n","                    dim=1,\n","                ).long()\n","            if inputs_embeds is None:\n","                inputs_embeds = self.word_embeddings(input_ids)\n","            prompts = self.get_prompt(batch_size=batch_size, task_ids=task_ids)\n","            #print(prompts.shape)#[1,175,768]\n","            prompts = prompts.to(inputs_embeds.dtype)\n","            #print(inputs_embeds.shape)#[1,4,175,1536]\n","            inputs_embeds = torch.cat((prompts.unsqueeze(0), inputs_embeds), dim=1)\n","            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n","\n","    def _prefix_tuning_forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        **kwargs,\n","    ):\n","        batch_size = _get_batch_size(input_ids, inputs_embeds)\n","        past_key_values = self.get_prompt(batch_size)\n","        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n","        kwargs.update(\n","            {\n","                \"input_ids\": input_ids,\n","                \"attention_mask\": attention_mask,\n","                \"inputs_embeds\": inputs_embeds,\n","                \"output_attentions\": output_attentions,\n","                \"output_hidden_states\": output_hidden_states,\n","                \"return_dict\": return_dict,\n","                \"past_key_values\": past_key_values,\n","            }\n","        )\n","        if \"past_key_values\" in fwd_params:\n","            return self.base_model(labels=labels, **kwargs)\n","        else:\n","            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n","            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n","            # if \"past_key_values\" not in fwd_params:\n","            #     raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n","            outputs = transformer_backbone_name(**kwargs)\n","            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n","            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n","                pooled_output = self.base_model.dropout(pooled_output)\n","            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n","\n","            loss = None\n","            if labels is not None:\n","                if self.config.problem_type is None:\n","                    if self.base_model.num_labels == 1:\n","                        self.config.problem_type = \"regression\"\n","                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                        self.config.problem_type = \"single_label_classification\"\n","                    else:\n","                        self.config.problem_type = \"multi_label_classification\"\n","\n","                if self.config.problem_type == \"regression\":\n","                    loss_fct = MSELoss()\n","                    if self.base_model.num_labels == 1:\n","                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                    else:\n","                        loss = loss_fct(logits, labels)\n","                elif self.config.problem_type == \"single_label_classification\":\n","                    loss_fct = CrossEntropyLoss()\n","                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n","                elif self.config.problem_type == \"multi_label_classification\":\n","                    loss_fct = BCEWithLogitsLoss()\n","                    loss = loss_fct(logits, labels)\n","            if not return_dict:\n","                output = (logits,) + outputs[2:]\n","                return ((loss,) + output) if loss is not None else output\n","\n","            return SequenceClassifierOutput(\n","                loss=loss,\n","                logits=logits,\n","                hidden_states=outputs.hidden_states,\n","                attentions=outputs.attentions,\n","            )"],"metadata":{"id":"0-espe9GUsYf","executionInfo":{"status":"ok","timestamp":1702645189618,"user_tz":-480,"elapsed":78,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from peft import PromptTuningConfig, get_peft_model, TaskType,PromptEmbedding\n","from peft import get_peft_config\n","config = {\n","       \"peft_type\": \"PROMPT_TUNING\",\n","       \"task_type\": \"SEQ_CLS\",\n","       \"inference_mode\": False,\n","       \"num_virtual_tokens\": 96,\n","       \"token_dim\": 1536,\n","       \"num_transformer_submodules\": 1,\n","       \"num_attention_heads\": 72,\n","       \"num_layers\": 72,\n","\n","\n","        }\n","peft_config = get_peft_config(config)\n","# peft_config = PromptTuningConfig(\n","#         num_virtual_tokens=10,\n","#         task_type=TaskType.SEQ_CLS\n","# )\n","#model = prepare_model_for_int8_training(model)\n","# 转换模型\n","peft_model = PeftModelForSequenceClassification(model, peft_config)\n","#model = get_peft_model(model, peft_config)\n","print(peft_model.print_trainable_parameters())\n","metric = evaluate.load(\"accuracy\")"],"metadata":{"id":"fS2gGw1krG3W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702645191418,"user_tz":-480,"elapsed":1878,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}},"outputId":"416bfbd8-6da6-4deb-c7d6-65f0300f9b4d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 148,993 || all params: 1,567,060,994 || trainable%: 0.009507798392689748\n","None\n"]}]},{"cell_type":"markdown","source":["# 新段落"],"metadata":{"id":"iKKwCUDaIXzz"}},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    remove_unused_columns=False,\n","    fp16=True,\n","    output_dir='./checkpoint',  # output directory\n","    num_train_epochs=5,  # total number of training epochs\n","    per_device_train_batch_size=1,  # batch size per device during training\n","    per_device_eval_batch_size=1,  # batch size for evaluation\n","    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,  # strength of weight decay\n","    logging_steps=100,\n","    save_strategy=\"no\",\n","    evaluation_strategy=\"epoch\",\n","    logging_dir='./logs',\n","\n",")\n","\n","trainer = Trainer(\n","        model=peft_model,  # the instantiated 🤗 Transformers model to be trained\n","        args=training_args,  # training arguments, defined above\n","        train_dataset=datasets.Dataset.from_dict(tokenized_train),  # training dataset\n","        eval_dataset=datasets.Dataset.from_dict(tokenized_val),  # evaluation dataset\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )"],"metadata":{"id":"evJjqmGUDNub","executionInfo":{"status":"ok","timestamp":1702645193949,"user_tz":-480,"elapsed":2549,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","prediction_outputs = trainer.predict(datasets.Dataset.from_dict(tokenized_test))\n","test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n","print(test_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"RYBfXc9ZDKDj","executionInfo":{"status":"ok","timestamp":1702645829172,"user_tz":-480,"elapsed":635238,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}},"outputId":"e89a7866-fd9f-4172-c6ca-f2946d408724"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1580' max='1580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1580/1580 10:11, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.602500</td>\n","      <td>1.599780</td>\n","      <td>0.275000</td>\n","      <td>0.232849</td>\n","      <td>0.288039</td>\n","      <td>0.275000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.624000</td>\n","      <td>1.623364</td>\n","      <td>0.187500</td>\n","      <td>0.201976</td>\n","      <td>0.239583</td>\n","      <td>0.187500</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.617900</td>\n","      <td>1.635364</td>\n","      <td>0.175000</td>\n","      <td>0.214195</td>\n","      <td>0.293631</td>\n","      <td>0.175000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.632600</td>\n","      <td>1.635217</td>\n","      <td>0.175000</td>\n","      <td>0.209235</td>\n","      <td>0.331984</td>\n","      <td>0.175000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.660300</td>\n","      <td>1.635449</td>\n","      <td>0.175000</td>\n","      <td>0.209235</td>\n","      <td>0.331984</td>\n","      <td>0.175000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[1 3 3 2 3 4 4 3 2 3 1 2 0 2 1 1 2 2 3 3 1 1 1 2 1 4 4 2 3 3 1 3 4 3 3 1 3\n"," 3 2 1 3 4 1 4 4 1 2 2 3 2 3 3 2 4 1 0 3 3 1 1 1 1 4 2 1 1 1 3 3 3 3 3 1 3\n"," 2 2 4 3 4 4 4 2 2 2 2 4 3 1 2 2 1 1 1 1 2 0 2 2 1 3 2 2 2 2 4 4 3 1 4 4 4\n"," 4 4 1 1 1 3 2 1 2]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AYnj4g1stLr_","executionInfo":{"status":"ok","timestamp":1702645829172,"user_tz":-480,"elapsed":21,"user":{"displayName":"Jie Wang","userId":"07776956424612082570"}}},"execution_count":11,"outputs":[]}]}